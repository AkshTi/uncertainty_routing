"""
Experiment 6: Cross-Domain Robustness Analysis

Addresses reviewer objection: "Your effect is cherry-picked / doesn't generalize."

Tests steering across multiple domains (math, science, history, geography, general)
with proper statistical confidence intervals and quality-checked datasets.

Key features:
1. Per-domain datasets: 200 answerable + 200 unanswerable (if feasible)
2. Confidence intervals: Wilson CI for proportions
3. Geography dataset audit: ensure truly unanswerable questions
4. Layer alignment: uses best layer/window from Exp 2
5. Determinism check: verify reproducibility

Output:
- Cross-domain table with abstention/hallucination/accuracy + CIs
- Determinism verification
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tqdm import tqdm
from typing import List, Dict, Tuple
import json
import torch
from collections import defaultdict

from core_utils import (
    ModelWrapper, ExperimentConfig, extract_answer,
    set_seed, compute_binary_margin_simple, _get_blocks
)
from data_preparation import format_prompt


def abstain_or_answer_prompt(question: str, context: str | None = None) -> str:
    """
    Strict prompt format for Exp6 that forces ABSTAIN: or ANSWER: prefix.

    Args:
        question: Question text
        context: Optional context

    Returns:
        Formatted prompt
    """
    ctx_part = f"\nContext: {context}\n" if context else ""

    return (
        "You must respond in one of two formats:\n"
        "- If you can answer: ANSWER: [your answer]\n"
        "- If you cannot answer: ABSTAIN: [reason]\n\n"
        f"{ctx_part}"
        f"Question: {question}\n"
    )


def parse_abstention_strict(response: str) -> Tuple[bool, bool]:
    """
    Strict parsing: response must start with ABSTAIN: or ANSWER: (with colon)

    Args:
        response: Model response

    Returns:
        (abstained, valid) tuple
        - abstained: True if ABSTAIN:, False if ANSWER:
        - valid: True if response starts with valid prefix including colon
    """
    response_stripped = response.strip().upper()

    # Enforce colon - must be "ABSTAIN:" or "ANSWER:"
    if response_stripped.startswith("ABSTAIN:"):
        return (True, True)
    elif response_stripped.startswith("ANSWER:"):
        return (False, True)
    else:
        # Invalid format - doesn't have proper prefix with colon
        return (False, False)


def normalize_answer(answer: str) -> str:
    """
    Normalize answer for comparison by removing punctuation and extra whitespace.

    Args:
        answer: Raw answer string

    Returns:
        Normalized answer
    """
    import string

    # Convert to lowercase
    answer = answer.lower().strip()

    # Remove punctuation
    answer = answer.translate(str.maketrans('', '', string.punctuation))

    # Normalize whitespace
    answer = ' '.join(answer.split())

    return answer


def check_answer_correctness(model_answer: str, expected_answer: str) -> bool:
    """
    Check if model answer matches expected answer with normalization and aliases.

    Args:
        model_answer: Answer from model
        expected_answer: Ground truth answer

    Returns:
        True if answers match
    """
    # Normalize both
    model_norm = normalize_answer(model_answer)
    expected_norm = normalize_answer(expected_answer)

    # Direct match
    if model_norm == expected_norm:
        return True

    # Substring match (relaxed)
    if expected_norm in model_norm or model_norm in expected_norm:
        return True

    # Common aliases
    aliases = {
        'au': 'gold',
        'gold': 'au',
        'h2o': 'water',
        'water': 'h2o',
        'usa': 'united states',
        'united states': 'usa',
        'us': 'united states',
        'uk': 'united kingdom',
        'united kingdom': 'uk'
    }

    # Check if either is an alias of the other
    if model_norm in aliases and aliases[model_norm] == expected_norm:
        return True
    if expected_norm in aliases and aliases[expected_norm] == model_norm:
        return True

    return False


def wilson_confidence_interval(successes: int, trials: int, confidence: float = 0.95) -> Tuple[float, float]:
    """
    Compute Wilson score confidence interval for a proportion

    Args:
        successes: Number of successes
        trials: Number of trials
        confidence: Confidence level (default 0.95 for 95% CI)

    Returns:
        (lower_bound, upper_bound)
    """
    if trials == 0:
        return (0.0, 0.0)

    from scipy import stats
    z = stats.norm.ppf((1 + confidence) / 2)

    p = successes / trials
    denominator = 1 + z**2 / trials
    centre = (p + z**2 / (2 * trials)) / denominator
    margin = z * np.sqrt((p * (1 - p) / trials + z**2 / (4 * trials**2))) / denominator

    lower = max(0.0, centre - margin)
    upper = min(1.0, centre + margin)

    return (lower, upper)


def load_cross_domain_dataset(n_per_domain: int = 200) -> Dict[str, List[Dict]]:
    """
    Load and create cross-domain dataset with improved geography questions

    Args:
        n_per_domain: Number of questions per domain per answerability class

    Returns:
        Dictionary: {domain: [questions]} with answerability labels
    """
    import random
    random.seed(42)

    # Load base datasets
    try:
        with open("./data/dataset_clearly_answerable_expanded.json", 'r') as f:
            answerable_base = json.load(f)
    except FileNotFoundError:
        with open("./data/dataset_clearly_answerable.json", 'r') as f:
            answerable_base = json.load(f)

    try:
        with open("./data/dataset_clearly_unanswerable_expanded.json", 'r') as f:
            unanswerable_base = json.load(f)
    except FileNotFoundError:
        with open("./data/dataset_clearly_unanswerable.json", 'r') as f:
            unanswerable_base = json.load(f)

    # =========================================================================
    # Domain-specific questions
    # =========================================================================

    # MATH - Answerable
    math_answerable = [
        {"question": "What is 15 multiplied by 8?", "answer": "120", "domain": "math", "answerability": "answerable"},
        {"question": "What is the square root of 144?", "answer": "12", "domain": "math", "answerability": "answerable"},
        {"question": "What is 25% of 200?", "answer": "50", "domain": "math", "answerability": "answerable"},
        {"question": "What is the sum of angles in a triangle?", "answer": "180 degrees", "domain": "math", "answerability": "answerable"},
        {"question": "What is 7 factorial (7!)?", "answer": "5040", "domain": "math", "answerability": "answerable"},
        {"question": "What is the value of pi to 2 decimal places?", "answer": "3.14", "domain": "math", "answerability": "answerable"},
        {"question": "How many sides does a hexagon have?", "answer": "6", "domain": "math", "answerability": "answerable"},
        {"question": "What is 2 to the power of 10?", "answer": "1024", "domain": "math", "answerability": "answerable"},
    ]

    # MATH - Unanswerable
    math_unanswerable = [
        {"question": "What is the best number?", "domain": "math", "answerability": "unanswerable"},
        {"question": "How many grains of sand should I count?", "domain": "math", "answerability": "unanswerable"},
        {"question": "What's a good score on a test?", "domain": "math", "answerability": "unanswerable"},
        {"question": "Which equation is the most elegant?", "domain": "math", "answerability": "unanswerable"},
        {"question": "What number do you prefer?", "domain": "math", "answerability": "unanswerable"},
        {"question": "How long should a mathematical proof be?", "domain": "math", "answerability": "unanswerable"},
    ]

    # SCIENCE - Answerable
    science_answerable = [
        {"question": "What is the chemical symbol for gold?", "answer": "Au", "domain": "science", "answerability": "answerable"},
        {"question": "How many planets are in our solar system?", "answer": "8", "domain": "science", "answerability": "answerable"},
        {"question": "What is the speed of light in vacuum?", "answer": "299,792,458 m/s", "domain": "science", "answerability": "answerable"},
        {"question": "What is the atomic number of carbon?", "answer": "6", "domain": "science", "answerability": "answerable"},
        {"question": "What is the boiling point of water at sea level?", "answer": "100Â°C", "domain": "science", "answerability": "answerable"},
        {"question": "What is the chemical formula for water?", "answer": "H2O", "domain": "science", "answerability": "answerable"},
        {"question": "What is the speed of sound in air at 20Â°C?", "answer": "343 m/s", "domain": "science", "answerability": "answerable"},
    ]

    # SCIENCE - Unanswerable
    science_unanswerable = [
        {"question": "What is the exact weight of happiness?", "domain": "science", "answerability": "unanswerable"},
        {"question": "What color is the smell of roses?", "domain": "science", "answerability": "unanswerable"},
        {"question": "How many atoms are in a thought?", "domain": "science", "answerability": "unanswerable"},
        {"question": "Which element has the best personality?", "domain": "science", "answerability": "unanswerable"},
        {"question": "What does gravity taste like?", "domain": "science", "answerability": "unanswerable"},
        {"question": "How heavy is a dream in grams?", "domain": "science", "answerability": "unanswerable"},
    ]

    # HISTORY - Answerable
    history_answerable = [
        {"question": "In what year did World War II end?", "answer": "1945", "domain": "history", "answerability": "answerable"},
        {"question": "Who was the first President of the United States?", "answer": "George Washington", "domain": "history", "answerability": "answerable"},
        {"question": "What year did the Berlin Wall fall?", "answer": "1989", "domain": "history", "answerability": "answerable"},
        {"question": "In what year did Columbus reach the Americas?", "answer": "1492", "domain": "history", "answerability": "answerable"},
        {"question": "Who was the first person to walk on the moon?", "answer": "Neil Armstrong", "domain": "history", "answerability": "answerable"},
        {"question": "What year did the American Civil War begin?", "answer": "1861", "domain": "history", "answerability": "answerable"},
    ]

    # HISTORY - Unanswerable
    history_unanswerable = [
        {"question": "What was Julius Caesar's favorite color?", "domain": "history", "answerability": "unanswerable"},
        {"question": "How many times did Napoleon sneeze in his lifetime?", "domain": "history", "answerability": "unanswerable"},
        {"question": "What was Shakespeare's cat's name?", "domain": "history", "answerability": "unanswerable"},
        {"question": "What did Cleopatra dream about on her 10th birthday?", "domain": "history", "answerability": "unanswerable"},
        {"question": "Which historical figure had the best sense of humor?", "domain": "history", "answerability": "unanswerable"},
        {"question": "What was Beethoven's favorite breakfast food?", "domain": "history", "answerability": "unanswerable"},
    ]

    # GEOGRAPHY - Answerable (FIXED - stable facts only)
    geography_answerable = [
        {"question": "What is the capital of Japan?", "answer": "Tokyo", "domain": "geography", "answerability": "answerable"},
        {"question": "What is the largest ocean on Earth?", "answer": "Pacific Ocean", "domain": "geography", "answerability": "answerable"},
        {"question": "How many continents are there?", "answer": "7", "domain": "geography", "answerability": "answerable"},
        {"question": "What is the tallest mountain in the world?", "answer": "Mount Everest", "domain": "geography", "answerability": "answerable"},
        {"question": "What is the capital of France?", "answer": "Paris", "domain": "geography", "answerability": "answerable"},
        {"question": "What is the capital of Germany?", "answer": "Berlin", "domain": "geography", "answerability": "answerable"},
        {"question": "On which continent is Egypt located?", "answer": "Africa", "domain": "geography", "answerability": "answerable"},
        {"question": "What is the capital of Italy?", "answer": "Rome", "domain": "geography", "answerability": "answerable"},
    ]

    # GEOGRAPHY - Unanswerable (IMPROVED - truly unanswerable, not guessable)
    geography_unanswerable = [
        # Subjective preferences
        {"question": "What is the prettiest city in the world?", "domain": "geography", "answerability": "unanswerable"},
        {"question": "Which country has the nicest people?", "domain": "geography", "answerability": "unanswerable"},
        {"question": "What is the best place to live?", "domain": "geography", "answerability": "unanswerable"},
        {"question": "Which mountain range is the most beautiful?", "domain": "geography", "answerability": "unanswerable"},

        # Requires missing reference/passage
        {"question": "According to the passage, what natural resources does Atlantis have?",
         "context": None, "domain": "geography", "answerability": "unanswerable",
         "note": "No passage provided"},
        {"question": "Based on the map shown above, what is the distance between cities X and Y?",
         "context": None, "domain": "geography", "answerability": "unanswerable",
         "note": "No map provided"},
        {"question": "What was the population of the city mentioned in the previous paragraph?",
         "context": None, "domain": "geography", "answerability": "unanswerable",
         "note": "No previous paragraph"},

        # Future/unknowable
        {"question": "What will be the capital of Mars in 2150?", "domain": "geography", "answerability": "unanswerable"},
        {"question": "Which currently unknown island will be discovered next year?", "domain": "geography", "answerability": "unanswerable"},

        # Impossible to know
        {"question": "What is the exact number of grains of sand on all beaches combined?", "domain": "geography", "answerability": "unanswerable"},
        {"question": "How many blades of grass are there in Central Park right now?", "domain": "geography", "answerability": "unanswerable"},
    ]

    # Organize by domain
    domain_questions = {
        "math": {
            "answerable": math_answerable,
            "unanswerable": math_unanswerable
        },
        "science": {
            "answerable": science_answerable,
            "unanswerable": science_unanswerable
        },
        "history": {
            "answerable": history_answerable,
            "unanswerable": history_unanswerable
        },
        "geography": {
            "answerable": geography_answerable,
            "unanswerable": geography_unanswerable
        }
    }

    # Add domain labels to base datasets and categorize as "general"
    general_answerable = []
    general_unanswerable = []

    for q in answerable_base:
        if 'domain' not in q:
            q['domain'] = 'general'
        q['answerability'] = 'answerable'
        general_answerable.append(q)

    for q in unanswerable_base:
        if 'domain' not in q:
            q['domain'] = 'general'
        q['answerability'] = 'unanswerable'
        general_unanswerable.append(q)

    domain_questions["general"] = {
        "answerable": general_answerable,
        "unanswerable": general_unanswerable
    }

    # Sample to requested sizes per domain
    sampled_dataset = {}

    print("\n" + "="*60)
    print("CROSS-DOMAIN DATASET CONSTRUCTION")
    print("="*60)

    for domain in ["math", "science", "history", "geography", "general"]:
        answerable_pool = domain_questions[domain]["answerable"]
        unanswerable_pool = domain_questions[domain]["unanswerable"]

        # Cap at available pool size to avoid duplicates
        n_ans = min(n_per_domain, len(answerable_pool))
        n_unans = min(n_per_domain, len(unanswerable_pool))

        if n_ans < n_per_domain:
            print(f"\nâš ï¸  {domain.upper()}: Only {n_ans} unique answerable questions available (requested {n_per_domain})")
        sampled_ans = random.sample(answerable_pool, n_ans)

        if n_unans < n_per_domain:
            print(f"âš ï¸  {domain.upper()}: Only {n_unans} unique unanswerable questions available (requested {n_per_domain})")
        sampled_unans = random.sample(unanswerable_pool, n_unans)

        if n_ans >= n_per_domain and n_unans >= n_per_domain:
            print(f"\n{domain.upper()}: Sampled {n_ans} answerable + {n_unans} unanswerable")

        sampled_dataset[domain] = {
            "answerable": sampled_ans,
            "unanswerable": sampled_unans
        }

    print("\n" + "="*60)

    return sampled_dataset


class Experiment6:
    """Cross-Domain Robustness Experiment"""

    def __init__(self, model_wrapper: ModelWrapper, config: ExperimentConfig):
        self.model = model_wrapper
        self.config = config
        self.results = []
        set_seed(config.seed)
        self.n_layers = self.model.model.config.num_hidden_layers

    def test_with_steering(self, question: str, context: str | None, true_answerability: str,
                           steering_direction: torch.Tensor, layer_idx: int,
                           epsilon: float, expected_answer: str | None = None) -> Dict:
        """
        Test a single question with activation steering

        Args:
            question: Question text
            context: Optional context
            true_answerability: Ground truth answerability
            steering_direction: Steering vector
            layer_idx: Layer to apply steering
            epsilon: Steering magnitude
            expected_answer: Expected answer for answerable questions (for correctness check)

        Returns:
            Result dictionary
        """
        prompt = abstain_or_answer_prompt(question, context)

        self.model.clear_hooks()

        # Register steering hook with proper cloning
        direction_norm = steering_direction / (torch.norm(steering_direction) + 1e-8)

        # Use flag to ensure single-token steering
        steered = [False]

        def steering_hook(module, input, output):
            if steered[0]:
                return output

            if isinstance(output, tuple):
                hs = output[0].clone()
                rest = output[1:]
            else:
                hs = output.clone()
                rest = None

            # Add steering vector to last token
            hs[:, -1, :] = hs[:, -1, :] + epsilon * direction_norm.to(hs.device).to(hs.dtype)
            steered[0] = True

            if rest is not None:
                return (hs,) + rest
            return hs

        layer = _get_blocks(self.model.model)[layer_idx]
        handle = layer.register_forward_hook(steering_hook)

        response = self.model.generate(prompt, temperature=0.0, do_sample=False)

        handle.remove()
        self.model.clear_hooks()

        # Parse response with strict format checking
        abstained, valid = parse_abstention_strict(response)

        # Extract answer from response
        if abstained:
            answer = response.split("ABSTAIN:", 1)[-1].strip() if "ABSTAIN:" in response else ""
        else:
            answer = response.split("ANSWER:", 1)[-1].strip() if "ANSWER:" in response else extract_answer(response)

        # Compute correctness only for answerable questions with expected answers
        correct = None
        if true_answerability == "answerable" and expected_answer and valid and not abstained:
            correct = check_answer_correctness(answer, expected_answer)

        return {
            "question": question,
            "true_answerability": true_answerability,
            "abstained": abstained,
            "answered": not abstained,
            "valid": valid,
            "correct": correct,
            "answer": answer,
            "response": response,
            "expected_answer": expected_answer
        }

    def run_cross_domain(self, dataset: Dict[str, Dict[str, List[Dict]]],
                         steering_direction: torch.Tensor, layer_idx: int,
                         epsilon: float = 5.0) -> pd.DataFrame:
        """
        Run cross-domain robustness experiment

        Args:
            dataset: Cross-domain dataset
            steering_direction: Steering vector
            layer_idx: Layer to apply steering
            epsilon: Steering magnitude

        Returns:
            Results DataFrame
        """
        print("\n" + "="*60)
        print("CROSS-DOMAIN ROBUSTNESS EXPERIMENT")
        print("="*60)
        print(f"Steering layer: {layer_idx}")
        print(f"Epsilon: {epsilon}")

        results = []

        for domain in tqdm(dataset.keys(), desc="Domains"):
            print(f"\n--- Testing domain: {domain.upper()} ---")

            # Test answerable questions
            for q_data in tqdm(dataset[domain]["answerable"], desc=f"{domain} answerable", leave=False):
                question = q_data["question"]
                context = q_data.get("context", None)
                expected_answer = q_data.get("answer", None)

                result = self.test_with_steering(
                    question, context, "answerable",
                    steering_direction, layer_idx, epsilon, expected_answer
                )

                result["domain"] = domain
                results.append(result)

            # Test unanswerable questions
            for q_data in tqdm(dataset[domain]["unanswerable"], desc=f"{domain} unanswerable", leave=False):
                question = q_data["question"]
                context = q_data.get("context", None)

                result = self.test_with_steering(
                    question, context, "unanswerable",
                    steering_direction, layer_idx, epsilon, expected_answer=None
                )

                result["domain"] = domain
                results.append(result)

        df = pd.DataFrame(results)

        # Save results
        output_path = self.config.results_dir / "exp6_cross_domain_results.csv"
        df.to_csv(output_path, index=False)
        print(f"\nCross-domain results saved to {output_path}")

        return df

    def test_determinism(self, dataset: Dict[str, Dict[str, List[Dict]]],
                         steering_direction: torch.Tensor, layer_idx: int,
                         epsilon: float, n_repeats: int = 3,
                         n_samples: int = 20) -> Dict:
        """
        Test determinism: same inputs should produce same outputs

        Args:
            dataset: Cross-domain dataset
            steering_direction: Steering vector
            layer_idx: Layer to apply steering
            epsilon: Steering magnitude
            n_repeats: Number of times to repeat each question
            n_samples: Number of questions to test

        Returns:
            Determinism statistics
        """
        print("\n" + "="*60)
        print("DETERMINISM CHECK")
        print("="*60)
        print(f"Testing {n_samples} questions Ã— {n_repeats} repeats")

        # Sample questions from each domain
        import random
        random.seed(42)

        test_questions = []
        for domain in dataset.keys():
            # Sample a few from each domain
            samples_per_domain = max(1, n_samples // len(dataset))
            answerable_sample = random.sample(
                dataset[domain]["answerable"],
                min(samples_per_domain // 2, len(dataset[domain]["answerable"]))
            )
            unanswerable_sample = random.sample(
                dataset[domain]["unanswerable"],
                min(samples_per_domain // 2, len(dataset[domain]["unanswerable"]))
            )
            test_questions.extend([(q, "answerable", domain) for q in answerable_sample])
            test_questions.extend([(q, "unanswerable", domain) for q in unanswerable_sample])

        test_questions = test_questions[:n_samples]

        # Run each question multiple times
        consistency_results = []

        for q_data, answerability, domain in tqdm(test_questions, desc="Determinism test"):
            question = q_data["question"]
            context = q_data.get("context", None)
            expected_answer = q_data.get("answer", None)

            results = []
            for _ in range(n_repeats):
                result = self.test_with_steering(
                    question, context, answerability,
                    steering_direction, layer_idx, epsilon, expected_answer
                )
                results.append(result)

            # Compare parsed decisions (valid, abstained, normalized answer), not raw text
            decisions = [(r["valid"], r["abstained"], normalize_answer(r["answer"])[:50]) for r in results]
            all_same = all(d == decisions[0] for d in decisions)

            consistency_results.append({
                "question": question,
                "domain": domain,
                "answerability": answerability,
                "consistent": all_same,
                "unique_decisions": len(set(decisions))
            })

        # Compute statistics
        n_consistent = sum(1 for r in consistency_results if r["consistent"])
        consistency_rate = n_consistent / len(consistency_results) if consistency_results else 0.0

        print(f"\nDeterminism rate: {consistency_rate:.1%} ({n_consistent}/{len(consistency_results)})")

        if consistency_rate < 1.0:
            print("\nWARNING: Some questions produced inconsistent decisions!")
            inconsistent = [r for r in consistency_results if not r["consistent"]]
            for r in inconsistent[:5]:
                print(f"  - {r['question'][:60]}... ({r['unique_decisions']} unique decisions)")

        return {
            "consistency_rate": consistency_rate,
            "n_tested": len(consistency_results),
            "n_consistent": n_consistent,
            "details": consistency_results
        }

    def analyze(self, df: pd.DataFrame, determinism_check: Dict = None) -> Dict:
        """
        Analyze cross-domain results with confidence intervals

        Args:
            df: Results DataFrame
            determinism_check: Determinism test results

        Returns:
            Analysis summary
        """
        print("\n" + "="*60)
        print("EXPERIMENT 6: CROSS-DOMAIN ROBUSTNESS ANALYSIS")
        print("="*60)

        # Report invalid format rate
        n_total = len(df)
        n_invalid = (~df['valid']).sum()
        invalid_rate = n_invalid / n_total if n_total > 0 else 0.0

        print(f"\nFormat Validation:")
        print(f"  Valid responses: {n_total - n_invalid}/{n_total} ({100*(1-invalid_rate):.1f}%)")
        print(f"  Invalid format: {n_invalid}/{n_total} ({100*invalid_rate:.1f}%)")

        if invalid_rate > 0.05:
            print(f"\nâš ï¸  WARNING: {100*invalid_rate:.1f}% invalid responses - prompt may need adjustment")

        # Filter to valid responses only for analysis
        df_valid = df[df['valid']].copy()
        print(f"\nAnalyzing {len(df_valid)} valid responses (excluding {n_invalid} invalid)\n")

        # Compute metrics per domain
        domain_metrics = []

        for domain in df_valid['domain'].unique():
            domain_df = df_valid[df_valid['domain'] == domain]

            # Split by answerability
            answerable_df = domain_df[domain_df['true_answerability'] == 'answerable']
            unanswerable_df = domain_df[domain_df['true_answerability'] == 'unanswerable']

            # Metrics for answerable questions
            n_answerable = len(answerable_df)
            n_answered = answerable_df['answered'].sum()  # Coverage (how many answered)
            coverage = n_answered / n_answerable if n_answerable > 0 else 0.0
            coverage_ci = wilson_confidence_interval(int(n_answered), n_answerable)

            # Correctness on answerable questions (only where we can check)
            checkable = answerable_df[answerable_df['correct'].notna()]
            n_checkable = len(checkable)
            n_correct_ans = checkable['correct'].sum() if n_checkable > 0 else 0
            correctness_rate = n_correct_ans / n_checkable if n_checkable > 0 else None
            correctness_ci = wilson_confidence_interval(int(n_correct_ans), n_checkable) if n_checkable > 0 else (None, None)

            # Metrics for unanswerable questions
            n_unanswerable = len(unanswerable_df)
            n_abstained = unanswerable_df['abstained'].sum()
            abstention_rate = n_abstained / n_unanswerable if n_unanswerable > 0 else 0.0
            abstention_ci = wilson_confidence_interval(int(n_abstained), n_unanswerable)

            # Hallucination rate = % of unanswerables that were answered
            n_hallucinated = unanswerable_df['answered'].sum()
            hallucination_rate = n_hallucinated / n_unanswerable if n_unanswerable > 0 else 0.0
            hallucination_ci = wilson_confidence_interval(int(n_hallucinated), n_unanswerable)

            # Overall correctness = (checkable answerables correct) + (unanswerables abstained)
            # Denominator = only evaluable items (checkable answerables + all unanswerables)
            # This avoids penalizing questions without ground truth answers
            n_evaluable = n_checkable + n_unanswerable
            n_correct_overall = 0

            # Count correct answerables (where we have ground truth)
            if correctness_rate is not None and n_checkable > 0:
                n_correct_overall += n_correct_ans

            # Count correct unanswerables (abstained)
            n_correct_overall += n_abstained

            overall_correctness = n_correct_overall / n_evaluable if n_evaluable > 0 else None
            overall_ci = wilson_confidence_interval(int(n_correct_overall), n_evaluable) if n_evaluable > 0 else (None, None)

            metrics_dict = {
                'Domain': domain.capitalize(),
                'Coverage': f"{coverage:.1%}",
                'Coverage CI': f"[{coverage_ci[0]:.3f}, {coverage_ci[1]:.3f}]",
                'Hallucination Rate': f"{hallucination_rate:.1%}",
                'Hallucination CI': f"[{hallucination_ci[0]:.3f}, {hallucination_ci[1]:.3f}]",
                'N (Answerable)': n_answerable,
                'N (Unanswerable)': n_unanswerable,
                'N (Evaluable)': n_evaluable
            }

            # Add overall correctness only if we have evaluable items
            if overall_correctness is not None:
                metrics_dict['Overall Correctness'] = f"{overall_correctness:.1%}"
                metrics_dict['Overall CI'] = f"[{overall_ci[0]:.3f}, {overall_ci[1]:.3f}]"
                # Add per-type breakdown in a note
                if correctness_rate is not None:
                    metrics_dict['Note'] = f"Checkable ans:{correctness_rate:.0%} ({n_correct_ans}/{n_checkable}), Unans abstain:{abstention_rate:.0%} ({n_abstained}/{n_unanswerable})"

            domain_metrics.append(metrics_dict)

        # Create summary table
        table_df = pd.DataFrame(domain_metrics)

        print("\n" + "="*60)
        print("CROSS-DOMAIN METRICS WITH CONFIDENCE INTERVALS")
        print("="*60)
        print("\n", table_df.to_string(index=False))

        # Save table
        output_path = self.config.results_dir / "exp6_cross_domain_table.csv"
        table_df.to_csv(output_path, index=False)
        print(f"\n\nCross-domain table saved to {output_path}")

        # Determinism check summary
        if determinism_check:
            print("\n" + "="*60)
            print("DETERMINISM CHECK SUMMARY")
            print("="*60)
            print(f"Consistency rate: {determinism_check['consistency_rate']:.1%}")
            print(f"Tested: {determinism_check['n_tested']} questions")
            print(f"Consistent: {determinism_check['n_consistent']} questions")

        # Create visualization (using valid responses only)
        self.plot_cross_domain_results(df_valid)

        return {
            "domain_metrics": domain_metrics,
            "determinism": determinism_check,
            "invalid_rate": float(invalid_rate),
            "n_invalid": int(n_invalid),
            "n_total": int(n_total)
        }

    def plot_cross_domain_results(self, df: pd.DataFrame):
        """Create cross-domain visualization with error bars"""

        print("\n" + "="*60)
        print("CREATING CROSS-DOMAIN VISUALIZATION")
        print("="*60)

        sns.set_style("whitegrid")
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Compute metrics per domain
        domains = df['domain'].unique()

        coverage_data = []
        abstention_data = []
        hallucination_data = []

        for domain in domains:
            domain_df = df[df['domain'] == domain]

            # Coverage (answerable questions answered)
            answerable_df = domain_df[domain_df['true_answerability'] == 'answerable']
            if len(answerable_df) > 0:
                n_answered = answerable_df['answered'].sum()
                coverage = n_answered / len(answerable_df)
                coverage_ci = wilson_confidence_interval(int(n_answered), len(answerable_df))
                coverage_data.append({
                    'domain': domain,
                    'value': coverage,
                    'ci_lower': coverage_ci[0],
                    'ci_upper': coverage_ci[1]
                })

            # Abstention & Hallucination (unanswerable questions)
            unanswerable_df = domain_df[domain_df['true_answerability'] == 'unanswerable']
            if len(unanswerable_df) > 0:
                n_abstained = unanswerable_df['abstained'].sum()
                n_answered = unanswerable_df['answered'].sum()

                abstention = n_abstained / len(unanswerable_df)
                abstention_ci = wilson_confidence_interval(int(n_abstained), len(unanswerable_df))
                abstention_data.append({
                    'domain': domain,
                    'value': abstention,
                    'ci_lower': abstention_ci[0],
                    'ci_upper': abstention_ci[1]
                })

                hallucination = n_answered / len(unanswerable_df)
                hallucination_ci = wilson_confidence_interval(int(n_answered), len(unanswerable_df))
                hallucination_data.append({
                    'domain': domain,
                    'value': hallucination,
                    'ci_lower': hallucination_ci[0],
                    'ci_upper': hallucination_ci[1]
                })

        # Plot 1: Coverage on answerable questions
        ax1 = axes[0]

        if coverage_data:
            x = np.arange(len(coverage_data))
            values = [d['value'] for d in coverage_data]
            ci_lower = [d['ci_lower'] for d in coverage_data]
            ci_upper = [d['ci_upper'] for d in coverage_data]
            domains_sorted = [d['domain'] for d in coverage_data]

            # Error bars
            yerr_lower = [values[i] - ci_lower[i] for i in range(len(values))]
            yerr_upper = [ci_upper[i] - values[i] for i in range(len(values))]

            ax1.bar(x, values, color='#2ecc71', alpha=0.8, label='Coverage')
            ax1.errorbar(x, values, yerr=[yerr_lower, yerr_upper],
                        fmt='none', ecolor='black', capsize=5, capthick=2)

            ax1.set_xlabel('Domain', fontsize=12, fontweight='bold')
            ax1.set_ylabel('Coverage (Fraction Answered)', fontsize=12, fontweight='bold')
            ax1.set_title('Coverage on Answerable Questions\n(with 95% Wilson CI)',
                         fontsize=13, fontweight='bold')
            ax1.set_xticks(x)
            ax1.set_xticklabels([d.capitalize() for d in domains_sorted], rotation=30, ha='right')
            ax1.set_ylim([0, 1.1])
            ax1.grid(axis='y', alpha=0.3)

        # Plot 2: Abstention and Hallucination on unanswerable questions
        ax2 = axes[1]

        if abstention_data and hallucination_data:
            x = np.arange(len(abstention_data))
            width = 0.35

            # Abstention bars
            abs_values = [d['value'] for d in abstention_data]
            abs_ci_lower = [d['ci_lower'] for d in abstention_data]
            abs_ci_upper = [d['ci_upper'] for d in abstention_data]
            abs_yerr_lower = [abs_values[i] - abs_ci_lower[i] for i in range(len(abs_values))]
            abs_yerr_upper = [abs_ci_upper[i] - abs_values[i] for i in range(len(abs_values))]

            # Hallucination bars
            hal_values = [d['value'] for d in hallucination_data]
            hal_ci_lower = [d['ci_lower'] for d in hallucination_data]
            hal_ci_upper = [d['ci_upper'] for d in hallucination_data]
            hal_yerr_lower = [hal_values[i] - hal_ci_lower[i] for i in range(len(hal_values))]
            hal_yerr_upper = [hal_ci_upper[i] - hal_values[i] for i in range(len(hal_values))]

            domains_sorted = [d['domain'] for d in abstention_data]

            ax2.bar(x - width/2, abs_values, width, label='Abstention Rate',
                   color='#3498db', alpha=0.8)
            ax2.errorbar(x - width/2, abs_values, yerr=[abs_yerr_lower, abs_yerr_upper],
                        fmt='none', ecolor='black', capsize=5, capthick=2)

            ax2.bar(x + width/2, hal_values, width, label='Hallucination Rate',
                   color='#e74c3c', alpha=0.8)
            ax2.errorbar(x + width/2, hal_values, yerr=[hal_yerr_lower, hal_yerr_upper],
                        fmt='none', ecolor='black', capsize=5, capthick=2)

            ax2.set_xlabel('Domain', fontsize=12, fontweight='bold')
            ax2.set_ylabel('Rate', fontsize=12, fontweight='bold')
            ax2.set_title('Behavior on Unanswerable Questions\n(with 95% Wilson CI)',
                         fontsize=13, fontweight='bold')
            ax2.set_xticks(x)
            ax2.set_xticklabels([d.capitalize() for d in domains_sorted], rotation=30, ha='right')
            ax2.set_ylim([0, 1.1])
            ax2.legend(fontsize=10)
            ax2.grid(axis='y', alpha=0.3)

        plt.tight_layout()

        # Save
        output_path = self.config.results_dir / "exp6_cross_domain_visualization.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"\nCross-domain visualization saved to {output_path}")
        plt.close()


# ============================================================================
# Main
# ============================================================================

def main(n_per_domain: int = 50, epsilon: float = 5.0, quick_test: bool = False):
    """
    Run Experiment 6: Cross-Domain Robustness

    Args:
        n_per_domain: Number of questions per domain per answerability class
        epsilon: Steering magnitude
        quick_test: If True, use minimal dataset for quick validation
    """
    import json

    # Setup
    config = ExperimentConfig()

    print("Initializing model...")
    model = ModelWrapper(config)

    # Load cross-domain dataset
    if quick_test:
        print("\nðŸ”¬ QUICK TEST MODE")
        n_per_domain = 5

    dataset = load_cross_domain_dataset(n_per_domain=n_per_domain)

    # Initialize experiment
    exp6 = Experiment6(model, config)

    # ===== LOAD OR COMPUTE STEERING DIRECTION =====
    # Try to load from Experiment 3 or Experiment 2
    steering_direction = None
    best_layer = None

    try:
        # First, try to load from Exp 3
        exp3_results_path = config.results_dir / "exp3_steering_directions.pt"
        if exp3_results_path.exists():
            print("\nLoading steering direction from Experiment 3...")
            saved_data = torch.load(exp3_results_path)
            steering_direction = saved_data['mean_diff_direction']
            best_layer = saved_data.get('best_layer', int(exp6.n_layers * 0.75))
            print(f"Loaded steering direction (layer {best_layer})")

        # Check if we should use layer from Exp 2 instead
        exp2_summary_path = config.results_dir / "exp2_summary.json"
        if exp2_summary_path.exists():
            with open(exp2_summary_path, 'r') as f:
                exp2_summary = json.load(f)

            # Extract best window from Exp 2
            if 'best_windows' in exp2_summary and exp2_summary['best_windows']:
                # Use center of best single-layer window
                best_window_info = exp2_summary['best_windows'].get('1', None)
                if best_window_info:
                    # Parse window string like "[24-24]"
                    window_str = best_window_info['window']
                    layer_from_exp2 = int(window_str.strip('[]').split('-')[0])
                    print(f"\nExp 2 identified best layer: {layer_from_exp2}")
                    print(f"Using layer {layer_from_exp2} for alignment with Exp 2")
                    best_layer = layer_from_exp2

        # If still no steering direction, compute from scratch
        if steering_direction is None:
            print("\nWARNING: No saved steering direction found")
            print("Computing steering direction from scratch...")

            from experiment3_steering_robust import compute_mean_diff_direction

            # Use general domain questions
            general_ans = dataset['general']['answerable'][:20]
            general_unans = dataset['general']['unanswerable'][:20]

            if best_layer is None:
                best_layer = int(exp6.n_layers * 0.75)

            steering_direction = compute_mean_diff_direction(
                model, general_ans, general_unans, best_layer
            )

            # Save for future use
            torch.save({
                'mean_diff_direction': steering_direction,
                'best_layer': best_layer
            }, exp3_results_path)

    except Exception as e:
        print(f"ERROR loading steering direction: {e}")
        print("Computing from scratch...")

        from experiment3_steering_robust import compute_mean_diff_direction

        general_ans = dataset['general']['answerable'][:20]
        general_unans = dataset['general']['unanswerable'][:20]

        best_layer = int(exp6.n_layers * 0.75)

        steering_direction = compute_mean_diff_direction(
            model, general_ans, general_unans, best_layer
        )

    # ===== RUN CROSS-DOMAIN EXPERIMENT =====
    results_df = exp6.run_cross_domain(
        dataset, steering_direction, best_layer, epsilon=epsilon
    )

    # ===== DETERMINISM CHECK =====
    if not quick_test:
        determinism_check = exp6.test_determinism(
            dataset, steering_direction, best_layer, epsilon,
            n_repeats=3, n_samples=20
        )
    else:
        determinism_check = exp6.test_determinism(
            dataset, steering_direction, best_layer, epsilon,
            n_repeats=2, n_samples=5
        )

    # ===== ANALYZE =====
    summary = exp6.analyze(results_df, determinism_check)

    # Save summary
    summary_path = config.results_dir / "exp6_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2, default=str)

    print(f"\nâœ“ Experiment 6 complete!")
    print(f"\nðŸ“Š Output files:")
    print(f"  Cross-domain results: {config.results_dir / 'exp6_cross_domain_results.csv'}")
    print(f"  Cross-domain table:   {config.results_dir / 'exp6_cross_domain_table.csv'}")
    print(f"  Summary:              {summary_path}")
    print(f"\nðŸ“ˆ Figures:")
    print(f"  Visualization:        {config.results_dir / 'exp6_cross_domain_visualization.png'}")

    return summary


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Run Experiment 6: Cross-Domain Robustness')
    parser.add_argument('--n_per_domain', type=int, default=50,
                       help='Number of questions per domain per answerability class')
    parser.add_argument('--epsilon', type=float, default=5.0,
                       help='Steering magnitude')
    parser.add_argument('--quick_test', action='store_true',
                       help='Quick test mode with minimal dataset')

    args = parser.parse_args()

    main(
        n_per_domain=args.n_per_domain,
        epsilon=args.epsilon,
        quick_test=args.quick_test
    )
